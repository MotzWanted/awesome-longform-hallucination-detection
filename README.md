# awesome-longform-hallucination-detection

Long-form text generation is widely used in large language models (LLMs), as referenced in studies [2, 3]. However, with longer texts, these models are more likely to generate nonsenical content, coined as "hallucinations", making it more challenging to evaluate [1]. 

One major issue is that benchmarks for detecting hallucinations in LLMs are often limited to short, factual questions and answers. Additionally, evaluating these hallucinations in long-form content is complex. The current methods for evaluation have their limitations, especially when dealing with complex, debatable facts or when different sources provide conflicting information. These challenges complicate the use of LLMs for applications that mimic real-world situations.

## Papers and Summaries

### [1] [FActScore: Fine-grained Atomic Evaluation of Factual Precision in Long Form Text Generation](https://arxiv.org/abs/2305.14251)

### [2] [Investigating Answerability of LLMs for Long-Form Question Answering](https://arxiv.org/abs/2309.08210)


### [3] [Understanding Retrieval Augmentation for Long-Form Question Answering](https://arxiv.org/abs/2310.12150)
